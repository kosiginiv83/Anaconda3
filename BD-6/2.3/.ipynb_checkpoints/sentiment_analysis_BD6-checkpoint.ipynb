{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "## <center>Практическая работа по нейронным сетям\n",
    "\n",
    "В работе использовался материал: [Виталия Радченко](https://github.com/Yorko/mlcourse_open/), Ю. В. Рубцова (Построение корпуса текстов для настройки тонового классификатора // Программные продукты и системы, 2015, №1(109), –С.72-78), [Никиты Учителева](https://habrahabr.ru/company/dca/blog/274027/), [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Нейронные сети в решении задач сентимент-анализа\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "Для многих целей важно знать, что о пишут в интернете по тому или иному вопросу. В работе рассмотрена задача классификация отзывов в интернете на позитивные и негативные, на языке машинного обучения это будет означать решение задачи бинарной классификации.\n",
    "\n",
    "## Данные\n",
    "\n",
    "В качестве источника текстов была выбрана платформа микроблогинга Twitter. Каждый текст в корпусе имеет следующие атрибуты:\n",
    "\n",
    "    – дата публикации;\n",
    "    – имя автора;\n",
    "    – текст твита;\n",
    "    – класс, к которому принадлежит текст (положительный, отрицательный, нейтральный);\n",
    "    – количество добавлений сообщения в избранное;\n",
    "    – количество ретвитов (количество копирований этого сообщения другими пользователями);\n",
    "    – количество друзей пользователя;\n",
    "    – количество пользователей, у которых данный юзер в друзьях (количество фоловеров);\n",
    "    – количество листов, в которых состоит пользователь.\n",
    "\n",
    " \n",
    "\n",
    "В результате используется тренировочный корпус, состоящий из 114,911 положительных, 111,923 отрицательных записей.\n",
    "\n",
    "\n",
    "## Подходы к решению\n",
    "\n",
    "Поскольку эта классическаязадача бинарной классификации, ее можно решать массой разных способов:\n",
    "- линейные модели ( логистическая регрессия, SVM, Naive Bayes и др.);\n",
    "- деревья, ансамбли, бустинг (дерево решений, случайный лес, Xgboost и др.);\n",
    "- Библиотека Facebook [FastText](https://github.com/facebookresearch/fastText);\n",
    "- нейронные сети на словах и символах (рекуррентные, LSTM, GRU, CNN и др.).\n",
    "\n",
    "Мы рассмотрим только решение, основанное на нейронных сетях. \n",
    "Итак, загрузим наши данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "positive = pd.read_csv('positive.csv', sep=';', header=None)\n",
    "negative = pd.read_csv('negative.csv', sep=';', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выгледят наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114906</th>\n",
       "      <td>411368729235054592</td>\n",
       "      <td>1386912922</td>\n",
       "      <td>diminlisenok</td>\n",
       "      <td>Спала в родительском доме, на своей кровати......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1497</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114907</th>\n",
       "      <td>411368729424187392</td>\n",
       "      <td>1386912922</td>\n",
       "      <td>qilepocagotu</td>\n",
       "      <td>RT @jebesilofyt: Эх... Мы немного решили сокра...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>692</td>\n",
       "      <td>225</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114908</th>\n",
       "      <td>411368796537257984</td>\n",
       "      <td>1386912938</td>\n",
       "      <td>DennyChooo</td>\n",
       "      <td>Что происходит со мной, когда в эфире #proacti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4905</td>\n",
       "      <td>448</td>\n",
       "      <td>193</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114909</th>\n",
       "      <td>411368797447417856</td>\n",
       "      <td>1386912938</td>\n",
       "      <td>bedowabymir</td>\n",
       "      <td>\"Любимая,я подарю тебе эту звезду...\" Имя како...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>989</td>\n",
       "      <td>254</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114910</th>\n",
       "      <td>411368857035898880</td>\n",
       "      <td>1386912953</td>\n",
       "      <td>Prituljak_Sibir</td>\n",
       "      <td>@Ma_che_rie посмотри #непытайтесьпокинутьомск ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1005</td>\n",
       "      <td>221</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0           1                2   \\\n",
       "114906  411368729235054592  1386912922     diminlisenok   \n",
       "114907  411368729424187392  1386912922     qilepocagotu   \n",
       "114908  411368796537257984  1386912938       DennyChooo   \n",
       "114909  411368797447417856  1386912938      bedowabymir   \n",
       "114910  411368857035898880  1386912953  Prituljak_Sibir   \n",
       "\n",
       "                                                       3   4   5   6   7   \\\n",
       "114906  Спала в родительском доме, на своей кровати......   1   0   0   0   \n",
       "114907  RT @jebesilofyt: Эх... Мы немного решили сокра...   1   0   1   0   \n",
       "114908  Что происходит со мной, когда в эфире #proacti...   1   0   0   0   \n",
       "114909  \"Любимая,я подарю тебе эту звезду...\" Имя како...   1   0   0   0   \n",
       "114910  @Ma_che_rie посмотри #непытайтесьпокинутьомск ...   1   0   0   0   \n",
       "\n",
       "          8    9    10  11  \n",
       "114906  1497   56   34   2  \n",
       "114907   692  225  210   0  \n",
       "114908  4905  448  193  13  \n",
       "114909   989  254  251   0  \n",
       "114910  1005  221  178   6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужна только колонка 3 в данных, которая содержит текст, а также метка с тональностью (1 - позитивно, 0 - негативно):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns =['text', 'sentiment'])\n",
    "data.loc[:, 'text'] = pd.concat([positive[3], negative[3]], axis=0, ignore_index=1)\n",
    "data.loc[0:len(positive)-1, 'sentiment'] = 1\n",
    "data.loc[len(positive):, 'sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0  @first_timee хоть я и школота, но поверь, у на...         1\n",
      "1  Да, все-таки он немного похож на него. Но мой ...         1\n",
      "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...         1\n",
      "3  RT @digger2912: \"Кто то в углу сидит и погибае...         1\n",
      "4  @irina_dyshkant Вот что значит страшилка :D\\nН...         1\n",
      "                                                     text sentiment\n",
      "226829  Но не каждый хочет что то исправлять:( http://...         0\n",
      "226830  скучаю так :-( только @taaannyaaa вправляет мо...         0\n",
      "226831          Вот и в школу, в говно это идти уже надо(         0\n",
      "226832  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...         0\n",
      "226833  Такси везет меня на работу. Раздумываю приплат...         0\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее надо немного обработать корпус и разбить выборку на тренировочную и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# инициализируем параметры \n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# приводим слова к нижнему регистру и убираем лишние символы\n",
    "data['text'] = data['text'].apply(lambda r: r.lower())\n",
    "data['text'] = data['text'].apply(lambda r: re.sub(r'[^а-я]+', ' ', r))\n",
    " \n",
    "# разделение выборки на тренировочную и тестовую\n",
    "data_train, data_test, label_train, label_test = \\\n",
    "    train_test_split(data['text'], data['sentiment'],\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0   хоть я и школота но поверь у нас то же самое ...         1\n",
      "1  да все таки он немного похож на него но мой ма...         1\n",
      "2                ну ты идиотка я испугалась за тебя          1\n",
      "3   кто то в углу сидит и погибает от голода а мы...         1\n",
      "4   вот что значит страшилка но блин посмотрев вс...         1\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных\n",
    "\n",
    "В \"сыром\" виде нельзя подавать данные в модель, их нужно правильно подготовить. \n",
    "Давайте заменим одинаковые слова на одинаковые числовые коды, редкие слова учитывать не будем, а также обрежем длину каждого предложения до 100 слов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложение до предобработки:\n",
      " как же мне нравятся мужчины в строгих костюмах млею \n",
      "Предложение после замены слов на индексы:\n",
      " [152, 19, 24, 61]\n",
      "Предложение после приведения к единой длинне:\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 152  19  24  61]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# инициализируем параметры словаря и эмбеддингов\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "print(\"Предложение до предобработки:\\n\", data_train[42])\n",
    "\n",
    "\n",
    "# с помощью Tokenizer создаем словарь \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "\n",
    "# заменяем слова на их индексы в нашем словаре\n",
    "X_train = tokenizer.texts_to_sequences(data_train)\n",
    "X_test = tokenizer.texts_to_sequences(data_test)\n",
    "\n",
    "print(\"Предложение после замены слов на индексы:\\n\", X_train[42])\n",
    "\n",
    "# обрезаем каждое предложение приводим к нужной длинне\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Предложение после приведения к единой длинне:\\n\", X_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Объект `Tokenizer` хранит в себе всю информацию про наш словарь. Нужно найти индекс слова \"сегодня\" и сколько раз оно встречалось в нашей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting re\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement re (from versions: )\n",
      "No matching distribution found for re\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install re #Запуск консольных команд\n",
    "tokenizer?? #Вывод исходников по чему угодно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tokenizer' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-3cf5669937e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ms_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'сегодня'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ms_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'сегодня'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Индекс слова 'сегодня' – {}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Слово 'сегодня' встречалось {} раз.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tokenizer' object does not support indexing"
     ]
    }
   ],
   "source": [
    "# ЗАМЕНИТЕ ?? НА ПРАВИЛЬНЫЙ ОТВЕТ\n",
    "s_index = tokenizer.word_index['сегодня']\n",
    "s_count = tokenizer.word_counts['сегодня']\n",
    "\n",
    "print(\"Индекс слова 'сегодня' – {}.\".format(s_index))\n",
    "print(\"Слово 'сегодня' встречалось {} раз.\".format(s_count))\n",
    "s_index = tokenizer.word_index['вчера']\n",
    "s_count = tokenizer.word_counts['вчера']\n",
    "print(\"Индекс слова 'вчера' – {}.\".format(s_index))\n",
    "print(\"Слово 'вчера' встречалось {} раз.\".format(s_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекуррентные нейронные сети (Recurrent neural networks, RNN)\n",
    "\n",
    "Рекуррентные нейронные сети помогают уловить/понять закономерность, которая зависит от времени или порядка. Например, когда мы пытаемся классифицировать какой-то эпизод из фильма, то нам важно знать что было пару эпизодов ранее, или чтобы понять смысл определенного слова, нам нужно знать контекст, который был до него.\n",
    "\n",
    "Простая рекуррентная нейронная сеть имеет следующее математическое представление:<br><br>\n",
    "$$\\large h_t = \\phi(Wx_t + Uh_{t-1})$$<br>\n",
    "$$\\large y = Vh_t$$\n",
    "\n",
    "Илюстрация к формуле:\n",
    "<img src=\"http://i.imgur.com/ifQrKRR.png\" alt=\"rnn\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 128)          12800000  \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               22900     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 12,823,001\n",
      "Trainable params: 12,823,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "max_features = 100000\n",
    "maxlen = 100 \n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 204150 samples, validate on 22684 samples\n",
      "Epoch 1/2\n",
      "204150/204150 [==============================] - 2033s - loss: 0.5698 - acc: 0.6994 - val_loss: 0.5332 - val_acc: 0.7349\n",
      "Epoch 2/2\n",
      "204150/204150 [==============================] - 1969s - loss: 0.4469 - acc: 0.7942 - val_loss: 0.5319 - val_acc: 0.7377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f29ea1aef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, label_train, validation_data=[X_test, label_test], \n",
    "          batch_size=32, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если натренировать данную модель, то точность на тренировочной выборке получится примерно 71.03%, а на валидации – примерно 74.59%.\n",
    "\n",
    "## Long short-term memory (LSTM)\n",
    "\n",
    "LSTM имеет ряд приемуществ над простой рекуррентной нейронной сетью. LSTM умеет хранить нужную информацию про определенный объект и не обращать внимание на неактуальную информацию. Например, сцена в книге без упоминания главного героя не будет менять информацию про него и, наоборот, при упоминании она будет фокусироваться. Рассмотрим на примере тренировки сети на тексте книги.\n",
    "\n",
    "- **Добавление механизма забывания.** Если эпизод книги заканчивается, то модель должна забыть текущее местоположение, время суток и сбросить любую информацию о конкретной сцене. Однако если персонаж книги умирает в сцене, сеть должна должна продолжать помнить, что он больше не жив. Таким образом, мы хотим, чтобы модель изучила отдельный механизм забывания/запоминания: когда появляются новые входные данные, она должна знать, какие факты сохранить или выбросить.\n",
    "\n",
    "- **Добавление механизма сохранения.** Когда модель увидит новую сцену, ей необходимо решить, стоит ли использовать и сохранять какую-либо информацию о ней. \n",
    "\n",
    "- Поэтому когда приходит новый вход, модель сначала забывает долгосрочную информацию, которая, как она решает, больше не нужна. Затем она узнает, какую часть новых данных стоит использовать, и сохраняет ее в своей долгострочной памяти.\n",
    "\n",
    "- **Фокусировка с долгосрочной памяти в рабочую память.** Наконец, модель должна узнать, какие части ее долговременной памяти сейчас полезны. Например, возраст героя может быть полезной информацией для сохранения в долгосрочной перспективе (дети с большей вероятностью будут ползать, взрослые скорее всего будут работать), но, вероятно, не имеет значения, если он не находится в текущей сцене. Таким образом, вместо того чтобы использовать полную долгосрочную память все время, она узнает, на каких частях стоит сосредоточиться.\n",
    "\n",
    "То есть преимущество LSTM над RNN в том, что RNN может только перезаписывать память, а LSTM более гибкая в этом плане и может хранить долгосрочную информацию, фокусируясь на нужных ее частях.\n",
    "\n",
    "Давайте рассмотрим игрушечный пример для закрепления понимания. \n",
    "\n",
    "Что \"думает\" полносвязная нейронная сеть:\n",
    "<img src=\"http://i.imgur.com/cOGzJxk.png\" alt=\"pokemon_nn\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "Что \"думает\" простая рекуррентная сеть:\n",
    "<img src=\"http://i.imgur.com/PnWiSCf.png\" alt=\"pokemon_rnn\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "Как видно из рисунка, рекуррентная сеть помнит, что случилось пару секунд назад, и может примерно понять, что стало причиной появления воды в следующем кадре.\n",
    "\n",
    "Что \"думает\" LSTM:\n",
    "<img src=\"http://i.imgur.com/EGZIUuc.png\" alt=\"pokemon_lstm\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "LSTM вспоминает, что было в предыдущем эпизоде, а также подтягивает долгосрочную память и фокусируется только на нужной для конкретного эпизода информации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, приступим к тренировке LSTM сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 128)          12800000  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100, 100)          91600     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100, 64)           42240     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 12,946,289\n",
      "Trainable params: 12,946,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 204150 samples, validate on 22684 samples\n",
      "Epoch 1/1\n",
      "204150/204150 [==============================] - 5572s - loss: 0.5346 - acc: 0.7290 - val_loss: 0.4966 - val_acc: 0.7550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2e5916dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Изменим архитектуру сети, добавив еще один слой.\n",
    "# Кроме того, будем отключать небольшую долю случайных нейронов сети для того,\n",
    "# чтобы она лучше обучалась: эта методика называется Dropuot\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# обратите внимание: для того, чтобы результат парвого слоя LSTM \n",
    "# использовать в следующем слое LSTM, необходимо добавить инструкцию\n",
    "# return_sequences=True\n",
    "\n",
    "model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train, label_train, validation_data=[X_test, label_test],  \n",
    "          batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При замене простой RNN на более сложную LSTM точность на тренировочной выборке возросла до 73.03%, а на валидации – до 75.70%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Попробуйте, варьируя параметры нейронной сети, поднять точность модели. Какой точности вы смогли добиться на валидационной выборке?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Другие варианты улучшения точности для LSTM модели в задаче сентимент анализа:\n",
    "- использование эмбеддингов;\n",
    "- увеличение размерности выхода ячейки LSTM;\n",
    "- на больших данных работает увеличение к-ва слоев;\n",
    "- переход от маленького батча к большому в процессе обучения;\n",
    "- подбор гиперпараметров для дропаута, регуляризации и оптимизатора.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще лучше с задачей сентимент-анализа справляются сверточные сети.\n",
    "У сверточных нейронных сетей есть несколько преимуществ перед LSTM:\n",
    "- не нужно хранить тысячи слов, а только небольшое к-во символов;\n",
    "- опечатки практически не влияют на точность модели (\"the bst film\" будет классифицирован как очень хороший, а LSTM просто проигнорирует данное слово).\n",
    "Однакое в данной работе мы ограничимся знакомством только с рекурентными сетями. \n",
    "\n",
    "Практические наблюдения:\n",
    "- если мало данных и отзывы короткие, то лучше использовать линейные методы (логистическая регрессия / SVM / etc);\n",
    "- если мало данных, но отзывы длинные – хорошо подойдет однослойная LSTM;\n",
    "- много данных – стоит пробовать разные архитектуры сети LSTM, CNN, а так же их модификации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим нашу модель в действии!\n",
    "Для этого нужно повторить все шаги подготовки, которые переводят текст в последовательность чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48138702"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_text = 'не очень хорошо'\n",
    "test_text = tokenizer.texts_to_sequences([your_text])\n",
    "test_train = pad_sequences(test_text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model.predict(test_train)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам может показаться этот пример простым, но в нем есть большая сложность для нейронной сети: она должна научиться праввильно понимать отрицание. Как мы можем видеть, приведенный пример она понимает корректно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "- Предложите натренированной сети несколько выражений из словаря [Эллочки-людоедки](https://ru.wikipedia.org/wiki/%D0%AD%D0%BB%D0%BB%D0%BE%D1%87%D0%BA%D0%B0-%D0%BB%D1%8E%D0%B4%D0%BE%D0%B5%D0%B4%D0%BA%D0%B0). Учтите, что сеть тренировалась на словах в нижнем регистре и состоящих только из букв, без дефисов и других дополнительных символов. Как Вы считаете, сможет ли сеть понять Эллочку? \n",
    "- Приведите пример выражений, которые сеть точно понимает неправильно. Объясните, почему сеть не смогла понять правильно найденные примеры?\n",
    "- Попробуйте подобрать максимально положительное и отрицательное выражение с точки зрения нейронной сети. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43734446"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_text = ['хамите', 'хохо', 'знаменито', 'мрачный', 'мрак', 'жуть', 'парниша', \\\n",
    "             'не учите меня жить', 'как ребенка', 'красота', 'толстый и красивый', 'поедем на извозчике', \\\n",
    "             'поедем на таксо', 'у вас вся спина белая', 'подумаешь', 'уля', 'ого']\n",
    "test_text = tokenizer.texts_to_sequences(your_text)\n",
    "test_train = pad_sequences(test_text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model.predict(test_train)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "l[:] = model.predict(test_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_n = []\n",
    "l_n[:] = [l[x][0] for x in range(len(l))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'жуть': 0.098267816,\n",
       " 'знаменито': 0.43734446,\n",
       " 'как ребенка': 0.41095126,\n",
       " 'красота': 0.81179899,\n",
       " 'мрак': 0.1983036,\n",
       " 'мрачный': 0.36615846,\n",
       " 'не учите меня жить': 0.25198126,\n",
       " 'ого': 0.51668906,\n",
       " 'парниша': 0.40984684,\n",
       " 'подумаешь': 0.55293721,\n",
       " 'поедем на извозчике': 0.50897479,\n",
       " 'поедем на таксо': 0.50897479,\n",
       " 'толстый и красивый': 0.54601938,\n",
       " 'у вас вся спина белая': 0.23567097,\n",
       " 'уля': 0.66065389,\n",
       " 'хамите': 0.43734446,\n",
       " 'хохо': 0.73016167}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "for i in range(17):\n",
    "    d[your_text[i]] = l_n[i]\n",
    "    \n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведите пример выражений, которые сеть точно понимает неправильно. Объясните, почему сеть не смогла понять правильно найденные примеры?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33427796"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_text = ['любить до одури']\n",
    "test_text = tokenizer.texts_to_sequences(your_text)\n",
    "test_train = pad_sequences(test_text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model.predict(test_train)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'да нет наверное'-0.25412351, 'ужасно красиво'-0.23244673, 'отлично все поломалось'-0.69717205, 'любить до одури'-0.33427796\n",
    "Сочетания слов с положительным и отрицательным окрасом, устойчивые выражения, которые сложно понять буквально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте подобрать максимально положительное и отрицательное выражение с точки зрения нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01580791]\n"
     ]
    }
   ],
   "source": [
    "print(min(model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.98136395]\n"
     ]
    }
   ],
   "source": [
    "print(max(model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0 37736     5    11  2262     2   266\n",
      "    49    77     1   416]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215081    0\n",
      "220754    0\n",
      "60463     1\n",
      "213263    0\n",
      "113055    1\n",
      "217274    0\n",
      "12325     1\n",
      "5133      1\n",
      "51187     1\n",
      "48452     1\n",
      "129119    0\n",
      "194817    0\n",
      "203181    0\n",
      "24705     1\n",
      "179983    0\n",
      "137859    0\n",
      "38434     1\n",
      "196691    0\n",
      "14144     1\n",
      "216746    0\n",
      "107498    1\n",
      "146303    0\n",
      "193715    0\n",
      "141388    0\n",
      "63352     1\n",
      "30918     1\n",
      "124347    0\n",
      "149921    0\n",
      "73194     1\n",
      "108348    1\n",
      "         ..\n",
      "199883    0\n",
      "14868     1\n",
      "144824    0\n",
      "143187    0\n",
      "142803    0\n",
      "97995     1\n",
      "24824     1\n",
      "145776    0\n",
      "67166     1\n",
      "207331    0\n",
      "36583     1\n",
      "176237    0\n",
      "222968    0\n",
      "77098     1\n",
      "196038    0\n",
      "148098    0\n",
      "28850     1\n",
      "224185    0\n",
      "132802    0\n",
      "79301     1\n",
      "66542     1\n",
      "119948    0\n",
      "7818      1\n",
      "37398     1\n",
      "199332    0\n",
      "8419      1\n",
      "126597    0\n",
      "154427    0\n",
      "198530    0\n",
      "83199     1\n",
      "Name: sentiment, Length: 22684, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0815874 ]\n",
      " [ 0.11275882]\n",
      " [ 0.427495  ]\n",
      " ..., \n",
      " [ 0.41582626]\n",
      " [ 0.34300584]\n",
      " [ 0.91161358]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
