{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "## <center>Практическая работа по нейронным сетям\n",
    "\n",
    "В работе использовался материал: [Виталия Радченко](https://github.com/Yorko/mlcourse_open/), Ю. В. Рубцова (Построение корпуса текстов для настройки тонового классификатора // Программные продукты и системы, 2015, №1(109), –С.72-78), [Никиты Учителева](https://habrahabr.ru/company/dca/blog/274027/), [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Нейронные сети в решении задач сентимент-анализа\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "Для многих целей важно знать, что о пишут в интернете по тому или иному вопросу. В работе рассмотрена задача классификация отзывов в интернете на позитивные и негативные, на языке машинного обучения это будет означать решение задачи бинарной классификации.\n",
    "\n",
    "## Данные\n",
    "\n",
    "В качестве источника текстов была выбрана платформа микроблогинга Twitter. Каждый текст в корпусе имеет следующие атрибуты:\n",
    "\n",
    "    – дата публикации;\n",
    "    – имя автора;\n",
    "    – текст твита;\n",
    "    – класс, к которому принадлежит текст (положительный, отрицательный, нейтральный);\n",
    "    – количество добавлений сообщения в избранное;\n",
    "    – количество ретвитов (количество копирований этого сообщения другими пользователями);\n",
    "    – количество друзей пользователя;\n",
    "    – количество пользователей, у которых данный юзер в друзьях (количество фоловеров);\n",
    "    – количество листов, в которых состоит пользователь.\n",
    "\n",
    " \n",
    "\n",
    "В результате используется тренировочный корпус, состоящий из 114,911 положительных, 111,923 отрицательных записей.\n",
    "\n",
    "\n",
    "## Подходы к решению\n",
    "\n",
    "Поскольку эта классическаязадача бинарной классификации, ее можно решать массой разных способов:\n",
    "- линейные модели ( логистическая регрессия, SVM, Naive Bayes и др.);\n",
    "- деревья, ансамбли, бустинг (дерево решений, случайный лес, Xgboost и др.);\n",
    "- Библиотека Facebook [FastText](https://github.com/facebookresearch/fastText);\n",
    "- нейронные сети на словах и символах (рекуррентные, LSTM, GRU, CNN и др.).\n",
    "\n",
    "Мы рассмотрим только решение, основанное на нейронных сетях. \n",
    "Итак, загрузим наши данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "positive = pd.read_csv('positive.csv', sep=';', header=None)\n",
    "negative = pd.read_csv('negative.csv', sep=';', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выгледят наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужна только колонка 3 в данных, которая содержит текст, а также метка с тональностью (1 - позитивно, 0 - негативно):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns =['text', 'sentiment'])\n",
    "data.loc[:, 'text'] = pd.concat([positive[3], negative[3]], axis=0, ignore_index=1)\n",
    "data.loc[0:len(positive)-1, 'sentiment'] = 1\n",
    "data.loc[len(positive):, 'sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее надо немного обработать корпус и разбить выборку на тренировочную и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# инициализируем параметры \n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# приводим слова к нижнему регистру и убираем лишние символы\n",
    "data['text'] = data['text'].apply(lambda r: r.lower())\n",
    "data['text'] = data['text'].apply(lambda r: re.sub(r'[^а-я]+', ' ', r))\n",
    " \n",
    "# разделение выборки на тренировочную и тестовую\n",
    "data_train, data_test, label_train, label_test = \\\n",
    "    train_test_split(data['text'], data['sentiment'],\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных\n",
    "\n",
    "В \"сыром\" виде нельзя подавать данные в модель, их нужно правильно подготовить. \n",
    "Давайте заменим одинаковые слова на одинаковые числовые коды, редкие слова учитывать не будем, а также обрежем длину каждого предложения до 100 слов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# инициализируем параметры словаря и эмбеддингов\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "print(\"Предложение до предобработки:\\n\", data_train[42])\n",
    "\n",
    "\n",
    "# с помощью Tokenizer создаем словарь \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "\n",
    "# заменяем слова на их индексы в нашем словаре\n",
    "X_train = tokenizer.texts_to_sequences(data_train)\n",
    "X_test = tokenizer.texts_to_sequences(data_test)\n",
    "\n",
    "print(\"Предложение после замены слов на индексы:\\n\", X_train[42])\n",
    "\n",
    "# обрезаем каждое предложение приводим к нужной длинне\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Предложение после приведения к единой длинне:\\n\", X_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Объект `Tokenizer` хранит в себе всю информацию про наш словарь. Нужно найти индекс слова \"сегодня\" и сколько раз оно встречалось в нашей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ЗАМЕНИТЕ ?? НА ПРАВИЛЬНЫЙ ОТВЕТ\n",
    "print(\"Индекс слова 'сегодня' – {}.\".format('??'))\n",
    "print(\"Слово 'сегодня' встречалось {} раз.\".format('??'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекуррентные нейронные сети (Recurrent neural networks, RNN)\n",
    "\n",
    "Рекуррентные нейронные сети помогают уловить/понять закономерность, которая зависит от времени или порядка. Например, когда мы пытаемся классифицировать какой-то эпизод из фильма, то нам важно знать что было пару эпизодов ранее, или чтобы понять смысл определенного слова, нам нужно знать контекст, который был до него.\n",
    "\n",
    "Простая рекуррентная нейронная сеть имеет следующее математическое представление:<br><br>\n",
    "$$\\large h_t = \\phi(Wx_t + Uh_{t-1})$$<br>\n",
    "$$\\large y = Vh_t$$\n",
    "\n",
    "Илюстрация к формуле:\n",
    "<img src=\"http://i.imgur.com/ifQrKRR.png\" alt=\"rnn\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "max_features = 100000\n",
    "maxlen = 100 \n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, label_train, validation_data=[X_test, label_test], \n",
    "          batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если натренировать данную модель, то точность на тренировочной выборке получится примерно 71.03%, а на валидации – примерно 74.59%.\n",
    "\n",
    "## Long short-term memory (LSTM)\n",
    "\n",
    "LSTM имеет ряд приемуществ над простой рекуррентной нейронной сетью. LSTM умеет хранить нужную информацию про определенный объект и не обращать внимание на неактуальную информацию. Например, сцена в книге без упоминания главного героя не будет менять информацию про него и, наоборот, при упоминании она будет фокусироваться. Рассмотрим на примере тренировки сети на тексте книги.\n",
    "\n",
    "- **Добавление механизма забывания.** Если эпизод книги заканчивается, то модель должна забыть текущее местоположение, время суток и сбросить любую информацию о конкретной сцене. Однако если персонаж книги умирает в сцене, сеть должна должна продолжать помнить, что он больше не жив. Таким образом, мы хотим, чтобы модель изучила отдельный механизм забывания/запоминания: когда появляются новые входные данные, она должна знать, какие факты сохранить или выбросить.\n",
    "\n",
    "- **Добавление механизма сохранения.** Когда модель увидит новую сцену, ей необходимо решить, стоит ли использовать и сохранять какую-либо информацию о ней. \n",
    "\n",
    "- Поэтому когда приходит новый вход, модель сначала забывает долгосрочную информацию, которая, как она решает, больше не нужна. Затем она узнает, какую часть новых данных стоит использовать, и сохраняет ее в своей долгострочной памяти.\n",
    "\n",
    "- **Фокусировка с долгосрочной памяти в рабочую память.** Наконец, модель должна узнать, какие части ее долговременной памяти сейчас полезны. Например, возраст героя может быть полезной информацией для сохранения в долгосрочной перспективе (дети с большей вероятностью будут ползать, взрослые скорее всего будут работать), но, вероятно, не имеет значения, если он не находится в текущей сцене. Таким образом, вместо того чтобы использовать полную долгосрочную память все время, она узнает, на каких частях стоит сосредоточиться.\n",
    "\n",
    "То есть преимущество LSTM над RNN в том, что RNN может только перезаписывать память, а LSTM более гибкая в этом плане и может хранить долгосрочную информацию, фокусируясь на нужных ее частях.\n",
    "\n",
    "Давайте рассмотрим игрушечный пример для закрепления понимания. \n",
    "\n",
    "Что \"думает\" полносвязная нейронная сеть:\n",
    "<img src=\"http://i.imgur.com/cOGzJxk.png\" alt=\"pokemon_nn\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "Что \"думает\" простая рекуррентная сеть:\n",
    "<img src=\"http://i.imgur.com/PnWiSCf.png\" alt=\"pokemon_rnn\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "Как видно из рисунка, рекуррентная сеть помнит, что случилось пару секунд назад, и может примерно понять, что стало причиной появления воды в следующем кадре.\n",
    "\n",
    "Что \"думает\" LSTM:\n",
    "<img src=\"http://i.imgur.com/EGZIUuc.png\" alt=\"pokemon_lstm\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "LSTM вспоминает, что было в предыдущем эпизоде, а также подтягивает долгосрочную память и фокусируется только на нужной для конкретного эпизода информации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, приступим к тренировке LSTM сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Изменим архитектуру сети, добавив еще один слой.\n",
    "# Кроме того, будем отключать небольшую долю случайных нейронов сети для того,\n",
    "# чтобы она лучше обучалась: эта методика называется Dropuot\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# обратите внимание: для того, чтобы результат парвого слоя LSTM \n",
    "# использовать в следующем слое LSTM, необходимо добавить инструкцию\n",
    "# return_sequences=True\n",
    "\n",
    "model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train, label_train, validation_data=[X_test, label_test],  \n",
    "          batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При замене простой RNN на более сложную LSTM точность на тренировочной выборке возросла до 73.03%, а на валидации – до 75.70%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Попробуйте, варьируя параметры нейронной сети, поднять точность модели. Какой точности вы смогли добиться на валидационной выборке?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Другие варианты улучшения точности для LSTM модели в задаче сентимент анализа:\n",
    "- использование эмбеддингов;\n",
    "- увеличение размерности выхода ячейки LSTM;\n",
    "- на больших данных работает увеличение к-ва слоев;\n",
    "- переход от маленького батча к большому в процессе обучения;\n",
    "- подбор гиперпараметров для дропаута, регуляризации и оптимизатора.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще лучше с задачей сентимент-анализа справляются сверточные сети.\n",
    "У сверточных нейронных сетей есть несколько преимуществ перед LSTM:\n",
    "- не нужно хранить тысячи слов, а только небольшое к-во символов;\n",
    "- опечатки практически не влияют на точность модели (\"the bst film\" будет классифицирован как очень хороший, а LSTM просто проигнорирует данное слово).\n",
    "Однакое в данной работе мы ограничимся знакомством только с рекурентными сетями. \n",
    "\n",
    "Практические наблюдения:\n",
    "- если мало данных и отзывы короткие, то лучше использовать линейные методы (логистическая регрессия / SVM / etc);\n",
    "- если мало данных, но отзывы длинные – хорошо подойдет однослойная LSTM;\n",
    "- много данных – стоит пробовать разные архитектуры сети LSTM, CNN, а так же их модификации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим нашу модель в действии!\n",
    "Для этого нужно повторить все шаги подготовки, которые переводят текст в последовательность чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "your_text = 'не очень хорошо'\n",
    "test_text = tokenizer.texts_to_sequences([your_text])\n",
    "test_train = pad_sequences(test_text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model.predict(test_train)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам может показаться этот пример простым, но в нем есть большая сложность для нейронной сети: она должна научиться праввильно понимать отрицание. Как мы можем видеть, приведенный пример она понимает корректно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "- Предложите натренированной сети несколько выражений из словаря [Эллочки-людоедки](https://ru.wikipedia.org/wiki/%D0%AD%D0%BB%D0%BB%D0%BE%D1%87%D0%BA%D0%B0-%D0%BB%D1%8E%D0%B4%D0%BE%D0%B5%D0%B4%D0%BA%D0%B0). Учтите, что сеть тренировалась на словах в нижнем регистре и состоящих только из букв, без дефисов и других дополнительных символов. Как Вы считаете, сможет ли сеть понять Эллочку? \n",
    "- Приведите пример выражений, которые сеть точно понимает неправильно. Объясните, почему сеть не смогла понять правильно найденные примеры?\n",
    "- Попробуйте подобрать максимально положительное и отрицательное выражение с точки зрения нейронной сети. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
